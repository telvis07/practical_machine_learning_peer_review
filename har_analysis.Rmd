---
title: 'Practical Machine Learning: Prediction of exercise type using body sensor
  data'
author: "Telvis Calhoun"
date: "March 17, 2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

## Executive Summary

TODO:

## Data Preparation
First, lets load libraries and datasets used in the analysis.

```{r, message=FALSE}
library(knitr)
library(dplyr)
library(randomForest)
library(caret)
library(rpart)
library(reshape2)
library(grid)
library(gridExtra)
data("mtcars")
```


First we remove columns that have all NAs. Next we use [rfImpute](http://www.inside-r.org/packages/cran/randomforest/docs/rfImpute) to fill in NA values.

```{r echo=FALSE,cache=TRUE}
training <- readRDS("data/pml_training_csv.rds")

# data cols
allcols <- names(training)
train_measure_cols <- allcols[grep("belt|arm|dumbbell",allcols)]
train_label_cols <- allcols[-grep("belt|arm|dumbbell",allcols)]

# get only the columns with sensor data
df <- subset(training, select=train_measure_cols)

# find columns where all the sensor data is null.
all_null_cols <- train_measure_cols[apply(df, 2, function(x) all(is.na(x)))]
train_measure_cols_not_null <- setdiff(train_measure_cols, all_null_cols)

# get only measurement columns with non-null values
df <- subset(training, select=train_measure_cols_not_null)

#
df$classe <- as.factor(training$classe)

# summary
total_variables_in_training_set <- length(names(training))
total_variables_with_belt_arm_dumbell_data <- length(train_measure_cols)
total_variables_with_nonnull_belt_arm_dumbell_data <- length(train_measure_cols_not_null)
total_rows_with_na_data <- sum(apply(df, 1, function(x) any(is.na(x))))

df <- data.frame(value=c(total_variables_with_belt_arm_dumbell_data, 
                          length(all_null_cols),
                          nrow(training),
                          total_rows_with_na_data))

rownames(df) <- c("total_variables_with_belt_arm_dumbell_data", 
                  "number_variables_all_missing",
                  "total_rows",
                  "total_rows_with_missing_data")

kable(df, row.names=TRUE)
```

## Feature Selection
We calculate feature importance the [varImp function](http://www.inside-r.org/packages/cran/randomforest/docs/importance) provided by the randomForest package.

```{r echo=FALSE,cache=TRUE}
set.seed(33833)

modFit <- readRDS("data/rf_fit_variable_importance.rds")

# find most important features
vi <- varImp(modFit, scale=FALSE)$importance
vi <- data.frame(varname=row.names(vi), Overall=vi$Overall)
vi <- vi[order(vi$Overall, decreasing = TRUE),]
rf_important_varnames <- vi[vi$Overall > 1,]$varname

# plot the top features
varImpPlot(modFit$finalModel, 
           scale=FALSE, 
           n.var=15,
           main=sprintf("Random Forest Variable Importance Plot \nof Top 15 of %s Variables", length(rf_important_varnames)))
```

## Model Selection


Random Forest model with 100 trees. We tried models with all 156 features and 36 features returned found in feature selection. We split the training in data in to 75% training and 25% testing.

```{r echo=TRUE,cache=TRUE}
set.seed(1234)
  
# read datums
modFit <- readRDS("data/rf_fit_all_features.rds")
df_imputed <- readRDS("data/rf_imputed_training_df.rds")

# split
inTrain = createDataPartition(df_imputed$classe, p = 3/4)[[1]]
training = df_imputed[ inTrain,]
testing = df_imputed[-inTrain,] 
```

The figure below show the confusion matrices for models built with all 156 predictors and the best 34 prediction with GINI greater than 1.0. The accuracy for model with all features is TODO. However the model with the best 36 features performs just as well.

```{r echo=FALSE,cache=TRUE,fig.align='center'}
set.seed(1234)

### Random Forest with all features
prediction_rf <- predict(modFit, testing)
confuse <- confusionMatrix(prediction_rf, testing$classe)
confuse.percent <- apply(confuse$table, 1, function(x) x/sum(x))
confuse.melt <- melt(confuse.percent)
confuse.df <- as.data.frame(confuse.melt)

p1 <- ggplot(confuse.df, aes(x=Reference, y=Prediction, fill=value)) +
  geom_tile(aes(fill=value)) +
  geom_text(aes(label = round(value, 3)), size=3) +
  scale_fill_gradient(low="white",high="red") +
  ggtitle("156 predictors") +
  xlab("Actual Class") +
  ylab("Predicted Class") 

### Random Forest with Best features
set.seed(1234)

# read datums
vi <- readRDS("data/rf_variable_importance_df.rds")
modFit_vi <- readRDS("data/rf_fit_36_features.rds")
df_imputed <- readRDS("data/rf_imputed_training_df.rds")
gini_threshold = 1

# modfit with most important features  
rf_important_varnames <- vi[vi$Overall > gini_threshold,]$varname
reduced_training <- subset(training, select=c(as.vector(rf_important_varnames), "classe"))
reduced_testing <- subset(testing, select=c(as.vector(rf_important_varnames), "classe"))

# testing
prediction_rf_vi <- predict(modFit_vi, reduced_testing)
confuse <- confusionMatrix(prediction_rf_vi, reduced_testing$classe)
confuse.percent <- apply(confuse$table, 1, function(x) x/sum(x))
confuse.melt <- melt(confuse.percent)
confuse.df <- as.data.frame(confuse.melt)

p2 <- ggplot(confuse.df, aes(x=Reference, y=Prediction, fill=value)) +
  geom_tile(aes(fill=value)) +
  geom_text(aes(label = round(value, 3)), size=3) +
  scale_fill_gradient(low="white",high="red") +
  ggtitle("Best 34 predictors") +
  xlab("Actual Class") +
  ylab("Predicted Class")

grid.arrange(p1, p2, ncol = 2, top="Normalized Confusion Matrix for Random Forest models")
```

## Prediction with Test Data

The test data contains several columns with missing data. We use the cleaned data to impute the missing values using [na.roughfix](http://www.inside-r.org/packages/cran/randomforest/docs/na.roughfix). 

TODO: Put stats here about the missing data in pml-testing.

```{r echo=FALSE,cache=TRUE}
pml_testing <- readRDS("data/pml_testing_csv.rds")

# summary
total_columns_with_all_na_data <- sum(apply(pml_testing, 2, function(x) all(is.na(x))))
total_rows_with_NA <- sum(apply(pml_testing, 1, function(x) any(is.na(x))))
total_columns <- ncol(pml_testing)
total_rows <- nrow(pml_testing)

# [1] "Number of Variables in Training Data: 160"
# [1] "Number of Belt, Arm, Dumbell Sensor Variables: 152"
# [1] "Number of Belt, Arm, Dumbell Variables with non-NA values: 146"
# [1] "Number of outcome variables (classe): 1"
# [1] "Number of rows where a variables are non-NA 217"
df <- data.frame(value=c(total_columns,
                        total_columns_with_all_na_data,
                         total_rows,
                          total_rows_with_NA))

rownames(df) <- c("total_columns",
                  "total_columns_with_all_missing_data", 
                  "total_rows", 
                  "total_rows_with_missing_data")

kable(df, row.names=TRUE, caption="pml_testing_csv summary")
```

## Conclusion

TODO:

# Appendix

## Predictions for pml_testing data

```{r echo=TRUE,cache=TRUE}
pml_testing <- readRDS("data/pml_testing_csv.rds")
  
# read datums
vi <- readRDS("data/rf_variable_importance_df.rds")
modFit_vi <- readRDS("data/rf_fit_36_features.rds")
df_imputed <- readRDS("data/rf_imputed_training_df.rds")
gini_threshold = 1

# data cols
allcols <- names(pml_testing)
train_measure_cols <- allcols[grep("belt|arm|dumbbell",allcols)]
rf_important_varnames <- vi[vi$Overall > gini_threshold,]$varname

# pml-testing data for 'most important' predictors
reduced_pml_testing <- subset(pml_testing, select=train_measure_cols)
reduced_pml_testing <- subset(pml_testing, select=c(as.vector(rf_important_varnames)))
reduced_pml_testing$problem_id <- pml_testing$problem_id


# training data for 'most important' predictors
reduced_df_imputed <- subset(df_imputed, select=c(as.vector(rf_important_varnames)))
reduced_df_imputed$classe <- df_imputed$classe


###########################################
# replace NA's in the test data.
# simply replace with the median for the column
# For numeric variables, NAs are replaced with column medians
reduced_pml_testing$is_test_data <- TRUE
reduced_df_imputed$is_test_data <- FALSE

# combine the data frames, remove 'classe' and 'problem_id' cols
combined_data <- rbind(subset(reduced_df_imputed, select=-c(classe)), subset(reduced_pml_testing, select=-c(problem_id)))
combined_rough <- na.roughfix(combined_data[,-length(combined_data)])
# get the roughfix data
combined_rough$is_test_data <- combined_data$is_test_data
pml_imputed <- filter(combined_rough, is_test_data==TRUE)

# predict on imputed pml-testing data
predict(modFit_vi, pml_imputed)
```


# Citations

[1] Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
Cited by 2 (Google Scholar)

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz439hx3Sdf

